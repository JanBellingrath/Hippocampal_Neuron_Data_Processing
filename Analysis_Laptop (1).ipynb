{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570c47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import dask\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from collections import namedtuple\n",
    "import mrestimator as mre\n",
    "\n",
    "import loren_frank_data_processing\n",
    "from loren_frank_data_processing.core import logger, get_epochs, get_data_structure, reconstruct_time\n",
    "from loren_frank_data_processing.tetrodes import get_trial_time, _get_tetrode_id\n",
    "from loren_frank_data_processing.neurons import make_neuron_dataframe, convert_neuron_epoch_to_dataframe, _add_to_dict, _get_neuron_id\n",
    "import loren_frank_data_processing.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98b9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Animal = namedtuple('Animal', {'directory', 'short_name'})\n",
    "conley = Animal('C:\\Conley', 'con')\n",
    "bond = Animal('C:\\Bond.tar\\Bond', 'bon')\n",
    "chapati = Animal('C:\\Chapati', 'cha')\n",
    "dudley = Animal('C:\\Dudley', 'dud')\n",
    "frank = Animal('C:\\Frank', 'fra')\n",
    "dave = Animal('C:\\Dave', 'dav')\n",
    "corriander = Animal('C:\\Corriander', 'Cor')\n",
    "animals = {'fra': Animal('C:\\Frank\\\\fra', 'fra'),'dud': Animal('C:\\Dudley\\\\dud', 'dud'), 'dav': Animal('C:\\Dave\\\\dav', 'dav'), 'con': Animal('C:\\Conley\\\\con', 'con'), 'Cor': Animal('Cor','C:\\Corriander\\\\Cor'),'cha': Animal('C:\\Chapati\\\\cha', 'cha'),'bon': Animal('C:\\Bond.tar\\Bond\\\\bon', 'bon')} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0affafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following functions from 'loren_frank_data_processing'got modified, with changes indicated\n",
    "def get_LFP_dataframe(tetrode_key, animals):\n",
    "    '''Gets the LFP data for a given epoch and tetrode.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tetrode_key : tuple\n",
    "        Unique key identifying the tetrode. Elements are\n",
    "        (animal_short_name, day, epoch, tetrode_number).\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "    Returns\n",
    "    -------\n",
    "    LFP : pandas dataframe\n",
    "        Contains the electric potential and time\n",
    "    '''\n",
    "    try:\n",
    "        lfp_file = loadmat(get_LFP_filename_modified(tetrode_key, animals))\n",
    "        lfp_data = lfp_file['eeg'][0, -1][0, -1][0, -1]\n",
    "        lfp_time = reconstruct_time(\n",
    "            lfp_data['starttime'][0, 0].item(),\n",
    "            lfp_data['data'][0, 0].size,\n",
    "            float(lfp_data['samprate'][0, 0].squeeze()))\n",
    "        return pd.Series(\n",
    "            data=lfp_data['data'][0, 0].squeeze().astype(float),\n",
    "            index=lfp_time,\n",
    "            name='{0}_{1:02d}_{2:02}_{3:03}'.format(*tetrode_key))\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        logger.warning('Failed to load file: {0}'.format(\n",
    "            get_LFP_filename_modified(tetrode_key, animals)))\n",
    "        \n",
    "def get_tetrode_info_path(animal):\n",
    "    '''Returns the Matlab tetrode info file name assuming it is in the\n",
    "    Raw Data directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    animal : namedtuple\n",
    "        First element is the directory where the animal's data is located.\n",
    "        The second element is the animal shortened name.\n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        The path to the information about the tetrodes for a given animal.\n",
    "    '''\n",
    "    #filename = '{animal.short_name}tetinfo.mat'.format(animal=animal)\n",
    "    filename = 'tetinfo.mat'\n",
    "    return join(animal.directory, filename)\n",
    "\n",
    "def make_tetrode_dataframe(animals, epoch_key=None):\n",
    "    \"\"\"Information about all tetrodes such as recording location.\n",
    "    Parameters\n",
    "    ----------\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "    Returns\n",
    "    -------\n",
    "    tetrode_infomation : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    tetrode_info = []\n",
    "    if epoch_key is not None:\n",
    "        animal, day, epoch = epoch_key\n",
    "        file_name = 'C:\\\\Frank\\\\fratetinfo.mat'\n",
    "        tet_info = loadmat(file_name, squeeze_me=True)[\"tetinfo\"]\n",
    "        tetrode_info.append(\n",
    "            convert_tetrode_epoch_to_dataframe(\n",
    "                tet_info[day - 1][epoch - 1], epoch_key))\n",
    "        return pd.concat(tetrode_info, sort=True)\n",
    "\n",
    "    for animal in animals.values():\n",
    "        file_name = get_tetrode_info_path(animal)\n",
    "        tet_info = loadmat(file_name, squeeze_me=True)[\"tetinfo\"]\n",
    "        try:\n",
    "            for day_ind, day in enumerate(tet_info):\n",
    "                try:\n",
    "                    for epoch_ind, epoch in enumerate(day):\n",
    "                        epoch_key = (\n",
    "                            animal.short_name,\n",
    "                            day_ind + 1,\n",
    "                            epoch_ind + 1,\n",
    "                        )  # noqa\n",
    "                        tetrode_info.append(\n",
    "                            convert_tetrode_epoch_to_dataframe(\n",
    "                                epoch, epoch_key)\n",
    "                        )\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        except TypeError:\n",
    "            # Only one day of recording\n",
    "            try:\n",
    "                day_ind = 0\n",
    "                for epoch_ind, epoch in enumerate(tet_info):\n",
    "                    epoch_key = animal.short_name, day_ind + 1, epoch_ind + 1\n",
    "                    tetrode_info.append(\n",
    "                        convert_tetrode_epoch_to_dataframe(epoch, epoch_key))\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return pd.concat(tetrode_info, sort=True)\n",
    "\n",
    "\n",
    "def get_LFP_filename_modified(tetrode_key, animals):\n",
    "\n",
    "    '''Returns a file name for the tetrode file LFP for an epoch.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tetrode_key : tuple\n",
    "        Unique key identifying the tetrode. Elements are\n",
    "        (animal_short_name, day, epoch, tetrode_number).\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        File path to tetrode file LFP\n",
    "    '''\n",
    "    animal, day, epoch, tetrode_number = tetrode_key\n",
    "    filename = ('{animal.short_name}eeg{day:02d}-{epoch}-'\n",
    "                '{tetrode_number:02d}.mat').format(\n",
    "                    animal=animals[animal], day=day, epoch=epoch,\n",
    "                    tetrode_number=tetrode_number)\n",
    "    if animal == 'con':\n",
    "        filename = 'C:\\Conley' + '\\EEG\\\\' + filename[-18:]\n",
    "        \n",
    "    if animal == 'bon':\n",
    "        filename = 'C:\\Bond.tar\\Bond' + filename[:-18] + '\\EEG\\\\' + filename[-18:]\n",
    "    \n",
    "    if animal == 'cha':\n",
    "        filename = 'C:\\Chapati' + filename[:-18] + '\\EEG\\\\' + filename[-18:]\n",
    "        \n",
    "    if animal == 'Cor':\n",
    "        filename = 'C:\\Corriander' + '\\EEG\\\\' + 'Cor'+ filename[-14:]\n",
    "        \n",
    "    if animal == 'dav':\n",
    "        filename = 'C:\\Dave' + filename[:-18] + '\\EEG\\\\' + filename[-18:]\n",
    "        \n",
    "    if animal == 'dud':\n",
    "        filename = filename[:-18] + '\\EEG\\\\' + filename[-18:] \n",
    "        \n",
    "    if animal == 'fra':\n",
    "        filename = filename[:-18] + '\\EEG\\\\' + filename[-18:] \n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "\n",
    "def get_trial_time(epoch_key, animals):\n",
    "    \"\"\"Time in the recording session in terms of the LFP.\n",
    "    This will return the LFP time of the first tetrode found (according to the\n",
    "    tetrode info). This is useful when there are slightly different timings\n",
    "    for the recordings and you need a common time.\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch_key : tuple\n",
    "        Unique key identifying a recording epoch with elements\n",
    "        (animal, day, epoch)\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "    Returns\n",
    "    -------\n",
    "    time : pandas.Index\n",
    "    \"\"\"\n",
    "    tetrode_info = make_tetrode_dataframe(animals, epoch_key=epoch_key)\n",
    "    for tetrode_key in tetrode_info.index:\n",
    "        lfp_df = get_LFP_dataframe(tetrode_key, animals)\n",
    "        if lfp_df is not None:\n",
    "            break\n",
    "\n",
    "    return lfp_df.index.rename(\"time\")\n",
    "\n",
    "\n",
    "def get_spikes_dataframe_mod(neuron_key, animals):\n",
    "    '''Spike times for a particular neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_key : tuple\n",
    "        Unique key identifying that neuron. Elements of the tuple are\n",
    "        (animal_short_name, day, epoch, tetrode_number, neuron_number).\n",
    "        Key can be retrieved from `make_neuron_dataframe` function.\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spikes_dataframe : pandas.DataFrame\n",
    "    '''\n",
    "    animal, day, epoch, tetrode_number, neuron_number = neuron_key\n",
    "    neuron_file = []\n",
    "    spike_time = []\n",
    "    try:\n",
    "        neuron_file = loadmat(\n",
    "            get_data_filename(animals[animal], day, 'spikes'))\n",
    "        \n",
    "    except (FileNotFoundError, TypeError):\n",
    "        logger.warning('Failed to load file: {0}'.format(\n",
    "            get_data_filename(animals[animal], day, 'spikes')))\n",
    "    if neuron_file != []:\n",
    "        try:\n",
    "            spike_time = neuron_file['spikes'][0, -1][0, epoch - 1][\n",
    "            0, tetrode_number - 1][0, neuron_number - 1][0]['data'][0][\n",
    "            :, 0]\n",
    "            spike_time = pd.TimedeltaIndex(spike_time, unit='s', name='time')\n",
    "        except IndexError:\n",
    "            spike_time = []\n",
    "        print(pd.Series(\n",
    "        np.ones_like(spike_time, dtype=int), index=spike_time,\n",
    "        name='{0}_{1:02d}_{2:02}_{3:03}_{4:03}'.format(*neuron_key)))\n",
    "    return pd.Series(\n",
    "        np.ones_like(spike_time, dtype=int), index=spike_time,\n",
    "        name='{0}_{1:02d}_{2:02}_{3:03}_{4:03}'.format(*neuron_key))\n",
    "\n",
    "def get_data_filename(animal, day, file_type):\n",
    "    '''Returns the Matlab file name assuming it is in the Raw Data\n",
    "    directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    animal : namedtuple\n",
    "        First element is the directory where the animal's data is located.\n",
    "        The second element is the animal shortened name.\n",
    "    day : int\n",
    "        Day of recording\n",
    "    file_type : str\n",
    "        Data structure name (e.g. linpos, dio)\n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        Path to data file\n",
    "    '''\n",
    "    filename = '{animal.short_name}{file_type}{day:02d}.mat'.format(\n",
    "        animal=animal,\n",
    "        file_type=file_type,\n",
    "        day=day)\n",
    "    #modify animal_path below\n",
    "    return join('C:\\\\Frank', filename)\n",
    "\n",
    "def convert_neuron_epoch_to_dataframe_modified(tetrodes_in_epoch, animal, day, epoch):\n",
    "    '''\n",
    "    Given an neuron data structure, return a cleaned up DataFrame\n",
    "    '''\n",
    "    DROP_COLUMNS = ['ripmodtag', 'thetamodtag', 'runripmodtag',\n",
    "                    'postsleepripmodtag', 'presleepripmodtag',\n",
    "                    'runthetamodtag', 'ripmodtag2', 'runripmodtag2',\n",
    "                    'postsleepripmodtag2', 'presleepripmodtag2',\n",
    "                    'ripmodtype', 'runripmodtype', 'postsleepripmodtype',\n",
    "                    'presleepripmodtype', 'FStag', 'ripmodtag3',\n",
    "                    'runripmodtag3', 'ripmodtype3', 'runripmodtype3',\n",
    "                    'tag', 'typetag', 'runripmodtype2',\n",
    "                    'tag2', 'ripmodtype2', 'descrip']\n",
    "\n",
    "    NEURON_INDEX = ['animal', 'day', 'epoch',\n",
    "                    'tetrode_number', 'neuron_number']\n",
    "\n",
    "    neuron_dict_list = [_add_to_dict(\n",
    "        _convert_to_dict(neuron), tetrode_ind, neuron_ind)\n",
    "        for tetrode_ind, tetrode in enumerate(\n",
    "        tetrodes_in_epoch[0][0])\n",
    "        for neuron_ind, neuron in enumerate(tetrode[0])\n",
    "        #if neuron.size > 0\n",
    "    ]\n",
    "    try:\n",
    "        return (pd.DataFrame(neuron_dict_list)\n",
    "                  .drop(DROP_COLUMNS, axis=1, errors='ignore')\n",
    "                  .assign(animal=animal)\n",
    "                \n",
    "                  .assign(day=day)\n",
    "                  .assign(epoch=epoch)\n",
    "                  .assign(neuron_id=_get_neuron_id\n",
    "                         )\n",
    "                # set index to identify rows\n",
    "                  .set_index(NEURON_INDEX)\n",
    "                  .sort_index())\n",
    "    except AttributeError:\n",
    "        logger.debug(f'Neuron info {animal}, {day}, {epoch} not processed')\n",
    "\n",
    "def _convert_to_dict_modified(struct_array):\n",
    "    try:\n",
    "        return {name: struct_array[name].item().item()\n",
    "                for name in struct_array.dtype.names\n",
    "                if struct_array[name].item().size == 1}\n",
    "    except TypeError:\n",
    "        return {}\n",
    "    #added in the AttributeError\n",
    "    except AttributeError:\n",
    "        return {}\n",
    "    \n",
    "def get_spike_indicator_dataframe_modified(neuron_key, animals, time_function=get_trial_time):\n",
    "    time = time_function(neuron_key[:3], animals)\n",
    "    spikes_df = get_spikes_dataframe_mod(neuron_key, animals)\n",
    "    time_index = None\n",
    "    try:\n",
    "        time_index = np.digitize(spikes_df.index.total_seconds(),\n",
    "                             time.total_seconds())\n",
    " \n",
    "        time_index[time_index >= len(time)] = len(time) -1\n",
    "    #the exception is for empty data\n",
    "    except AttributeError: \n",
    "        print('No spikes here; data is emtpy')\n",
    "    \n",
    "    #the following accounts for an empty time_index\n",
    "    if time_index is not None:\n",
    "        return (spikes_df.groupby(time[time_index]).sum().reindex(index=time, fill_value=0))\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def make_neuron_dataframe_modified(animals):\n",
    "    '''Information about all recorded neurons such as brain area.\n",
    "    The index of the dataframe corresponds to the unique key for that neuron\n",
    "    and can be used to load spiking information.\n",
    "    Parameters\n",
    "    ----------\n",
    "    animals : dict of named-tuples\n",
    "        Dictionary containing information about the directory for each\n",
    "        animal. The key is the animal_short_name.\n",
    "    Returns\n",
    "    -------\n",
    "    neuron_information : pandas.DataFrame\n",
    "    '''\n",
    "    #neuron_file_names = [(get_neuron_info_path(animals[animal]), animal)\n",
    "     #                    for animal in animals]\n",
    "    neuron_file_names = ['C:\\\\Frank\\\\fracellinfo.mat']#modify\n",
    "    neuron_data = [(loadmat(file_name), animal_name) for animal_name, file_name in zip(animals.keys(), neuron_file_names)]\n",
    "\n",
    "    return pd.concat([\n",
    "        convert_neuron_epoch_to_dataframe(\n",
    "            epoch, animal, day_ind + 1, epoch_ind + 1)\n",
    "        for cellfile, animal in neuron_data\n",
    "        for day_ind, day in enumerate(cellfile['cellinfo'].T)\n",
    "        for epoch_ind, epoch in enumerate(day[0].T)\n",
    "    ]).sort_index()\n",
    "\n",
    "#modified for access with correct name\n",
    "def get_tetrode_info_path_modified(animal):\n",
    "    '''Returns the Matlab tetrode info file name assuming it is in the\n",
    "    Raw Data directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    animal : namedtuple\n",
    "        First element is the directory where the animal's data is located.\n",
    "        The second element is the animal shortened name.\n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        The path to the information about the tetrodes for a given animal.\n",
    "    '''\n",
    "    filename = 'tetinfo'\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_tetrode_epoch_to_dataframe(tetrodes_in_epoch, epoch_key):\n",
    "    '''Convert tetrode information data structure to dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tetrodes_in_epoch : matlab data structure\n",
    "    epoch_key : tuple\n",
    "        Unique key identifying a recording epoch. Elements are\n",
    "        (animal, day, epoch)\n",
    "    Returns\n",
    "    -------\n",
    "    tetrode_info : dataframe\n",
    "    '''\n",
    "    animal, day, epoch = epoch_key\n",
    "    print(tetrodes_in_epoch)\n",
    "    tetrode_dict_list = [_convert_to_dict(\n",
    "        tetrode) for tetrode in tetrodes_in_epoch]\n",
    "    return (pd.DataFrame(tetrode_dict_list)\n",
    "              .assign(animal=lambda x: animal)\n",
    "              .assign(day=lambda x: day)\n",
    "              .assign(epoch=lambda x: epoch)\n",
    "              .assign(tetrode_number=lambda x: x.index + 1)\n",
    "              .assign(tetrode_id=_get_tetrode_id)\n",
    "            # set index to identify rows\n",
    "              .set_index(['animal', 'day', 'epoch', 'tetrode_number'])\n",
    "              .sort_index()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1232967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new functions \n",
    "\n",
    "\n",
    "#sums 2DArray of trials over time, over all trials, such that one cummulative time-series results\n",
    "def sum_time_series_dict(time_series_dict):\n",
    "    \n",
    "    num_bins = max([len(x) for x in time_series_dict.values() if x is not None])\n",
    "    res = np.zeros(num_bins)\n",
    "    for arr in time_series_dict.values():\n",
    "        if arr is not None:\n",
    "            res[:len(arr)] += arr\n",
    "    return res\n",
    "\n",
    "#generates dictionary of time-series, relative to neuron_key_list (indeces) given as input\n",
    "def generate_spike_indicator_dict(neuron_key_list, animals):\n",
    "    spike_indicator_dict = {}\n",
    "    for neuron_key_str in neuron_key_list:\n",
    "        animal_short_name, day_number, epoch_number, tetrode_number, neuron_number = neuron_key_str.split(\"_\")\n",
    "        neuron_key = (animal_short_name, int(day_number), int(epoch_number), int(tetrode_number), int(neuron_number))\n",
    "        try:\n",
    "            spike_indicator_array = get_spike_indicator_dataframe_modified(neuron_key, animals).values\n",
    "        except AttributeError:\n",
    "            spike_indicator_array = None\n",
    "            print(f\"No spike indicator data for neuron: {neuron_key}\")\n",
    "        spike_indicator_dict[neuron_key] = spike_indicator_array\n",
    "    return spike_indicator_dict\n",
    "\n",
    "\n",
    "def split_neuron_dataframe_informationally(df, split_cols):\n",
    "    '''Splits a DataFrame into multiple DataFrames based on specified\n",
    "    column values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to be split.\n",
    "    split_cols : list of str\n",
    "        The names of the columns to use for splitting the DataFrame.\n",
    "    Returns\n",
    "    -------\n",
    "    dfs : dict\n",
    "        A dictionary containing the split DataFrames. The keys are tuples\n",
    "        containing the unique combinations of the specified column values.\n",
    "    '''\n",
    "    dfs = {}\n",
    "    for idx, group in df.groupby(split_cols):\n",
    "        dfs[idx] = group.copy()\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def dict_to_numpy(my_dict):\n",
    "    my_array = np.array([v for v in my_dict.values()])\n",
    "    return my_array\n",
    "\n",
    "def dict_to_list(my_dict):\n",
    "    my_list = [v for v in my_dict.values() if v is not None]\n",
    "    return my_list\n",
    "\n",
    "def pad_arrays(arrays):\n",
    "    \"\"\"\n",
    "    Pads a list of arrays with zeros so that they are all the same length.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrays : list of numpy arrays\n",
    "        The arrays to pad.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    padded_arrays : numpy array\n",
    "        The padded arrays, stacked along the first axis.\n",
    "    \"\"\"\n",
    "    # Get the maximum length of all the arrays\n",
    "    max_len = max(len(arr) for arr in arrays if arr is not None)\n",
    "    \n",
    "    # Initialize an array of zeros to hold the padded arrays\n",
    "    padded_arrays = np.zeros((len(arrays), max_len))\n",
    "    \n",
    "    # Pad each array with zeros to make it the same length as the longest array\n",
    "    for i, arr in enumerate(arrays):\n",
    "        if arr is not None:\n",
    "            padded_arrays[i, :len(arr)] = arr\n",
    "    \n",
    "    return padded_arrays\n",
    "\n",
    "def _convert_to_dict(struct_array):\n",
    "    try:\n",
    "        return {name: struct_array[name].item().item()\n",
    "                for name in struct_array.dtype.names\n",
    "                if struct_array[name].item().size == 1}\n",
    "    except TypeError:\n",
    "        return {}\n",
    "    #added in the AttributeError\n",
    "    except AttributeError:\n",
    "        return {}\n",
    "    \n",
    "def chunk_array(arr, n):\n",
    "    # Determine the size of each subarray\n",
    "    size = (len(arr) + n - 1) // n\n",
    "\n",
    "    # Create the 2D list\n",
    "    result = [[] for _ in range(n)]\n",
    "\n",
    "    # Iterate over the original array and insert each element into the appropriate subarray\n",
    "    for i, elem in enumerate(arr):\n",
    "        subarray_index = i // size\n",
    "        result[subarray_index].append(elem)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3db93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('CA1', 'fra'), ('CA2', 'fra'), ('CA3', 'fra'), ('DG', 'fra')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = make_neuron_dataframe_modified(animals)\n",
    "splitted_df = split_neuron_dataframe_informationally(df, ['area', 'animal'])\n",
    "\n",
    "neuron_ids = df['neuron_id'].tolist()\n",
    "\n",
    "CA1 = splitted_df['CA1', 'fra']\n",
    "CA3 = splitted_df['CA3', 'fra']\n",
    "DG = splitted_df['DG', 'fra']\n",
    "\n",
    "list_id = CA1['neuron_id'].tolist()\n",
    "spike_dict_what = generate_spike_indicator_dict(list_id, animals)\n",
    "\n",
    "#printed_keys below are from informationally split df function above.keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b816b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spike_dict_what' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-077a0984a331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummed_time_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_time_series_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspike_dict_what\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spike_dict_what' is not defined"
     ]
    }
   ],
   "source": [
    "summed_time_series = sum_time_series_dict(spike_dict_what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fb0e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = chunk_array(summed_time_series, 100)\n",
    "chunks = chunks[:99]\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "886ddea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     input_handler() detected list, parsing to numpy ndarray as float\n",
      "INFO     input_handler() returning ndarray with 99 trial(s) and 22331 datapoints\n",
      "INFO     input_handler() detected ndarray of numbers\n",
      "INFO     input_handler() returning ndarray with 99 trial(s) and 22331 datapoints\n",
      "INFO     coefficients() with 'trialseparated' method for 99 trials of length 22331.\n",
      "INFO     Bootstrapping 5 replicas\n",
      "INFO     5 bootstrap replicas done\n",
      "INFO     Bounded fit to $|A| e^{-k/\\tau} + B e^{-(k/\\tau_{osc})^\\gamma} \\cos(2 \\pi \\nu k) + C e^{-(k/\\tau_{gs})^2} + O$\n",
      "INFO     Fitting with 22 different start values\n",
      "INFO     Bootstrapping 5 replicas (22 fits each)\n",
      "INFO     Finished fitting the data to f_complex,\n",
      "         mre = 0.99590(31), tau = 973.73(90.85)ms, ssres = 0.00140\n",
      "WARNING  The amplitude of the exponential decay is smaller than corrections: A=0.004426 B=-0.026857 C=0.007784\n",
      "INFO     Saving plot to C:\\Users\\janbe\\output/Full Analysis.pdf\n",
      "INFO     Saving meta to C:\\Users\\janbe\\output/Full Analysis.tsv\n",
      "INFO     full_analysis() done\n"
     ]
    }
   ],
   "source": [
    "#del spike_dict_CA3\n",
    "spike_dict_CA3 = mre.input_handler(chunks)\n",
    "#spike_dict_CA3 = mre.input_handler(padded_array)\n",
    "#print([np.isfinite(chunk) for chunk in spike_dict_CA3 if False])\n",
    "\n",
    "auto = mre.full_analysis(\n",
    "    \n",
    "    data= spike_dict_CA3,\n",
    "    method = 'ts',\n",
    "    numboot = 5,\n",
    "    targetdir='./output',\n",
    "    title='Full Analysis',\n",
    "    dt=4, dtunit='ms',\n",
    "    tmin=0, tmax=8000,\n",
    "    fitfuncs=['complex'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
